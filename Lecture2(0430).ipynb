{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DylanCTY/Trial-space/blob/main/Lecture2(0430).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing"
      ],
      "metadata": {
        "id": "mcDV8lrlsb_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "eTsD-Rfxs8Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer"
      ],
      "metadata": {
        "id": "jfLT5lbXtF1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download extra packages"
      ],
      "metadata": {
        "id": "pCRPyADTtSb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "XYprK53utM0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the file"
      ],
      "metadata": {
        "id": "22gQuGdMzvjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/sms_spam.csv')"
      ],
      "metadata": {
        "id": "ysli7eIXzy4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "TOLK7hEF1FoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing files from Github"
      ],
      "metadata": {
        "id": "tJYBZX-nBS08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def list_github_directory(user, repo, path):\n",
        "    url = f\"https://api.github.com/repos/{user}/{repo}/contents/{path}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        content = response.json()\n",
        "        return [file['name'] for file in content if file['type'] == 'file']\n",
        "    else:\n",
        "        print(\"Failed to retrieve data:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "# Usage\n",
        "user = 'RDGopal'\n",
        "repo = 'IB9CW0-Text-Analytics'\n",
        "path = 'Data'\n",
        "files = list_github_directory(user, repo, path)\n",
        "print(\"Files in the Data folder:\", files)\n"
      ],
      "metadata": {
        "id": "NnoVbw6BBXfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "nHGy52Uv1ljt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase\n",
        "df['text'] = df['text'].str.lower()"
      ],
      "metadata": {
        "id": "qKvtHACE1qlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "df['tokens'] = df['text'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "lzn7m_R15Sqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "extra_words = ['.','*',',']\n",
        "stop_words.extend(extra_words)"
      ],
      "metadata": {
        "id": "hen-yoKZ5pkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "df['tokens'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token not in stop_words and token.isalpha()])"
      ],
      "metadata": {
        "id": "CiMxr6Bu-NT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PorterStemmer object\n",
        "stemmer = PorterStemmer()\n",
        "df['tokens'] = df['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])"
      ],
      "metadata": {
        "id": "9dGi1FVz-tTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
      ],
      "metadata": {
        "id": "Lfz9iht_-891"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine tokens back into a cleaned review\n",
        "df['text1'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))"
      ],
      "metadata": {
        "id": "3ssLzou3_Yx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['text','text1']]"
      ],
      "metadata": {
        "id": "4yFdrmFB_rwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put it all into a function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    extra_words = ['.','*',',']\n",
        "    stop_words.extend(extra_words)\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "-ca8XWqCEzb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "-aicK2tZEbLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Your Turn\n",
        "Read and preprocess the file `oct_delta.csv`"
      ],
      "metadata": {
        "id": "aGGA0DpUHRGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pracctice 1"
      ],
      "metadata": {
        "id": "PNexNe_7lh5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('oct_delta.csv')"
      ],
      "metadata": {
        "id": "gB_CPFQulgm2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "1s3c1_BUlj91",
        "outputId": "8ead78b9-a1a7-4a8e-b76e-057ecd2ba0e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     weekday month  date  year  \\\n",
              "0        Thu   Oct     1  2015   \n",
              "1        Thu   Oct     1  2015   \n",
              "2        Thu   Oct     1  2015   \n",
              "3        Thu   Oct     1  2015   \n",
              "4        Thu   Oct     1  2015   \n",
              "...      ...   ...   ...   ...   \n",
              "1372     Thu   Oct    15  2015   \n",
              "1373     Thu   Oct    15  2015   \n",
              "1374     Thu   Oct    15  2015   \n",
              "1375     Thu   Oct    15  2015   \n",
              "1376     Thu   Oct    15  2015   \n",
              "\n",
              "                                                   text  \\\n",
              "0      i know that can be frustrating..we hope to ha...   \n",
              "1      terribly sorry for the inconvenience. if we c...   \n",
              "2       i can check, pls follow and dm your confirma...   \n",
              "3      ...alerts, pls check here: http://t.co/0jlczn...   \n",
              "4      ...advisory has only been issued for the baha...   \n",
              "...                                                 ...   \n",
              "1372   woohoo! way to go marla and mira! happy trave...   \n",
              "1373              you're welcome! have a great day! *rd   \n",
              "1374    if you do not make your connection, the gate...   \n",
              "1375                                  ...719pm. *dd 2/2   \n",
              "1376                          that sounds yummy. :) *cm   \n",
              "\n",
              "                                                 tokens  \n",
              "0     [know, frustrating, hope, parked, deplaned, sh...  \n",
              "1     [terribly, sorry, inconvenience, assistance, t...  \n",
              "2     [check, pls, follow, dm, confirmation, review,...  \n",
              "3                         [alert, pls, check, http, jh]  \n",
              "4     [advisory, issued, bahamas, could, change, che...  \n",
              "...                                                 ...  \n",
              "1372  [woohoo, way, go, marla, mira, happy, travel, dd]  \n",
              "1373                          [welcome, great, day, rd]  \n",
              "1374  [make, connection, gate, agent, advise, option...  \n",
              "1375                                               [dd]  \n",
              "1376                                 [sound, yummy, cm]  \n",
              "\n",
              "[1377 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fc1686f-3436-4596-89ff-d38cb32fc640\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>i know that can be frustrating..we hope to ha...</td>\n",
              "      <td>[know, frustrating, hope, parked, deplaned, sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>terribly sorry for the inconvenience. if we c...</td>\n",
              "      <td>[terribly, sorry, inconvenience, assistance, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>i can check, pls follow and dm your confirma...</td>\n",
              "      <td>[check, pls, follow, dm, confirmation, review,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>...alerts, pls check here: http://t.co/0jlczn...</td>\n",
              "      <td>[alert, pls, check, http, jh]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>...advisory has only been issued for the baha...</td>\n",
              "      <td>[advisory, issued, bahamas, could, change, che...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1372</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>15</td>\n",
              "      <td>2015</td>\n",
              "      <td>woohoo! way to go marla and mira! happy trave...</td>\n",
              "      <td>[woohoo, way, go, marla, mira, happy, travel, dd]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1373</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>15</td>\n",
              "      <td>2015</td>\n",
              "      <td>you're welcome! have a great day! *rd</td>\n",
              "      <td>[welcome, great, day, rd]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1374</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>15</td>\n",
              "      <td>2015</td>\n",
              "      <td>if you do not make your connection, the gate...</td>\n",
              "      <td>[make, connection, gate, agent, advise, option...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1375</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>15</td>\n",
              "      <td>2015</td>\n",
              "      <td>...719pm. *dd 2/2</td>\n",
              "      <td>[dd]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1376</th>\n",
              "      <td>Thu</td>\n",
              "      <td>Oct</td>\n",
              "      <td>15</td>\n",
              "      <td>2015</td>\n",
              "      <td>that sounds yummy. :) *cm</td>\n",
              "      <td>[sound, yummy, cm]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1377 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fc1686f-3436-4596-89ff-d38cb32fc640')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0fc1686f-3436-4596-89ff-d38cb32fc640 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0fc1686f-3436-4596-89ff-d38cb32fc640');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9dc310b2-01ea-4311-ae6e-36654669cfb7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9dc310b2-01ea-4311-ae6e-36654669cfb7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9dc310b2-01ea-4311-ae6e-36654669cfb7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2",
              "summary": "{\n  \"name\": \"df2\",\n  \"rows\": 1377,\n  \"fields\": [\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Thu\",\n          \"Fri\",\n          \"Tue\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Oct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 1,\n        \"max\": 15,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2015,\n        \"max\": 2015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1360,\n        \"samples\": [\n          \" ...will try to make this right. *bb 2/2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove usernames\n",
        "df2['text'] = df2['text'].apply(lambda x: re.sub(r'@\\w+', '', x))"
      ],
      "metadata": {
        "id": "4GqSrEM_oaMU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase\n",
        "df2['text'] = df2['text'].str.lower()"
      ],
      "metadata": {
        "id": "Bo48yiXgnZwF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "df2['tokens'] = df2['text'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "j4r2C6_RnZmx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "extra_words = ['.','*',',']\n",
        "stop_words.extend(extra_words)"
      ],
      "metadata": {
        "id": "LLUGmrxdnZcv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "df2['tokens'] = df2['tokens'].apply(lambda tokens: [token for token in tokens if token not in stop_words and token.isalpha()])"
      ],
      "metadata": {
        "id": "hYBT3XjlnZLc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df2['tokens'] = df2['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
      ],
      "metadata": {
        "id": "V9Uw7AYoljkz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-Words and tf-idf"
      ],
      "metadata": {
        "id": "cJez0UcIFf5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel,CoherenceModel,TfidfModel,Nmf,LsiModel"
      ],
      "metadata": {
        "id": "POsQsoJ4Fp3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of tokens\n",
        "documents = df['tokens'].tolist()"
      ],
      "metadata": {
        "id": "WWoTnFRgFsAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "784Z59uMF4Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dictionary\n",
        "dictionary = Dictionary(documents) # list of lists (documents)"
      ],
      "metadata": {
        "id": "7_amKNStGJs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional if want to see the dictionary\n",
        "dictionary.save_as_text('testxyz.csv',sort_by_word=True)"
      ],
      "metadata": {
        "id": "VWcvS4nZGLfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter extremes from the dictionary (optional, but recommended)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5) # at least appears in 5 documents, no more than 50%"
      ],
      "metadata": {
        "id": "zSN_KP9PHx8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create bag-of-words representation\n",
        "corpus = [dictionary.doc2bow(document) for document in documents]"
      ],
      "metadata": {
        "id": "Ac1RV8wVH9_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "CtoHhe3rID_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tf-idf representation\n",
        "tfidf_model = TfidfModel(corpus)\n",
        "tfidf_corpus = [tfidf_model[doc] for doc in corpus]"
      ],
      "metadata": {
        "id": "P7bpSUzcISSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_corpus"
      ],
      "metadata": {
        "id": "itbQf-3DIW07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency Analysis"
      ],
      "metadata": {
        "id": "lFGhVIaYx87X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib wordcloud\n"
      ],
      "metadata": {
        "id": "18kZkWaWyAI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform frequency analysis, we need to count how often each word appears in your corpus. We can utilize the dictionary and the Bag-of-Words (BoW) corpus"
      ],
      "metadata": {
        "id": "J5eyDC9jyTUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Summing up the counts from the BoW corpus\n",
        "total_word_count = Counter(word_id for document in corpus for word_id, count in document)\n",
        "\n",
        "# Mapping back the word IDs to words\n",
        "mapped_word_counts = [(dictionary[word_id], count) for word_id, count in total_word_count.items()]\n",
        "\n",
        "# Sort words by frequency\n",
        "sorted_word_counts = sorted(mapped_word_counts, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Let's plot the top words\n",
        "plt.figure(figsize=(10, 5))\n",
        "words, counts = zip(*sorted_word_counts[:40])\n",
        "plt.bar(words, counts)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Words by Frequency')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H8DZx-ZZyNaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Word Cloud\n",
        "To create a word cloud, you will need the frequencies in a dictionary format, where the keys are words and the values are their frequencies."
      ],
      "metadata": {
        "id": "JEweScxUy_qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Creating a dictionary for word cloud\n",
        "word_freq_dict = dict(sorted_word_counts)\n",
        "\n",
        "# Creating word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(word_freq_dict)\n",
        "\n",
        "# Displaying the word cloud\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-t_9AkV_zGhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Conduct frequency analysis with `oct_delta.csv` file."
      ],
      "metadata": {
        "id": "SMxY-w0RKSfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny Shakespeare novel"
      ],
      "metadata": {
        "id": "xO8hZeCr2fxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the file locally"
      ],
      "metadata": {
        "id": "F_vddh2JFG7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the text file\n",
        "with open('/content/tinyshakespeare.txt', 'r') as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n"
      ],
      "metadata": {
        "id": "6IKd9lig2os_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Or Read the file from GitHub"
      ],
      "metadata": {
        "id": "GswT33ABFK7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL to the raw text file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/RDGopal/IB9CW0-Text-Analytics/main/Data/tinyshakespeare.txt'\n",
        "\n",
        "# Use requests to get the content of the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if response.status_code == 200:\n",
        "    text = response.text.lower()\n",
        "    # Continue processing the text as needed\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "48_gKi3iEKpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "xmpZvKm52v1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zipf's Law\n",
        "\n",
        "Zipf's Law is an empirical law that suggests the frequency of a word in a natural language text is inversely proportional to its rank in the frequency table. To test Zipf's Law with your tokenized text data, we follow the steps below:\n",
        "\n",
        "1. **Calculate Word Frequencies**: Count how often each word appears in your tokenized text.\n",
        "\n",
        "2. **Sort Words by Frequency**: Rank the words by their frequency in descending order.\n",
        "\n",
        "3. **Plot the Frequencies**: Plot the frequency of each word against its rank on a log-log plot.\n",
        "\n",
        "4. **Analyze the Plot**: Zipf's Law predicts a linear relationship on a log-log plot with a slope of approximately -1.\n"
      ],
      "metadata": {
        "id": "JhkYl1iXGC9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Count frequencies\n",
        "word_counts = Counter(tokens)\n",
        "\n",
        "# Sort words by frequency\n",
        "sorted_word_counts = word_counts.most_common()\n",
        "\n",
        "# Prepare data for plotting\n",
        "ranks = range(1, len(sorted_word_counts) + 1)\n",
        "frequencies = [freq for _, freq in sorted_word_counts]\n",
        "\n",
        "# Log-log plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.loglog(ranks, frequencies, marker=\"o\")\n",
        "plt.title('Zipf\\'s Law')\n",
        "plt.xlabel('Rank of the word')\n",
        "plt.ylabel('Frequency of the word')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wLlP-MXPGHsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokens\n",
        "filtered_tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n"
      ],
      "metadata": {
        "id": "fsPogqVZ20m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_tokens)"
      ],
      "metadata": {
        "id": "BqhMHoTh29fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "D6PA1qlu3Xnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts"
      ],
      "metadata": {
        "id": "lNK67sFj3aHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(word_counts)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f_aFueIs3k_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the 20 most common words\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "# Unpack the words and their frequencies\n",
        "words, frequencies = zip(*most_common_words)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(words, frequencies)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Words')\n",
        "plt.xticks(rotation=45)  # Rotate the words on x-axis to avoid overlapping\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eRVInZzs36IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Run the following code to get a novel from HugginFace and conduct Zipf's law analysis."
      ],
      "metadata": {
        "id": "8c0SHY2sKiPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL to the raw text file on GitHub\n",
        "url =\"https://datasets-server.huggingface.co/rows?dataset=JiggaBooJombs%2FNovelist&config=default&split=train&offset=0&length=100\"\n",
        "\n",
        "# Use requests to get the content of the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if response.status_code == 200:\n",
        "    text = response.text.lower()\n",
        "    # Continue processing the text as needed\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "JEQIJ_qqKjky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging"
      ],
      "metadata": {
        "id": "kooyfJGl1Rck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "rDxee82w60Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# POS tagging the filtered tokens\n",
        "pos_tags = nltk.pos_tag(filtered_tokens)\n",
        "\n",
        "# Count word frequencies (including the POS tags for uniqueness)\n",
        "word_counts = Counter(pos_tags)\n",
        "\n",
        "# Create a list of dictionaries to later convert to a DataFrame\n",
        "data = [{'Word': word, 'POS': pos, 'WordCount': count} for (word, pos), count in word_counts.items()]\n",
        "\n",
        "# Create the DataFrame\n",
        "df_pos = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(df_pos.head())"
      ],
      "metadata": {
        "id": "09yMxD0-1TdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create POS mapping"
      ],
      "metadata": {
        "id": "s513SrJJE5Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag_full_form = {\n",
        "    'CC': 'Coordinating conjunction',\n",
        "    'CD': 'Cardinal number',\n",
        "    'DT': 'Determiner',\n",
        "    'EX': 'Existential there',\n",
        "    'FW': 'Foreign word',\n",
        "    'IN': 'Preposition or subordinating conjunction',\n",
        "    'JJ': 'Adjective',\n",
        "    'JJR': 'Adjective, comparative',\n",
        "    'JJS': 'Adjective, superlative',\n",
        "    'LS': 'List item marker',\n",
        "    'MD': 'Modal',\n",
        "    'NN': 'Noun, singular or mass',\n",
        "    'NNS': 'Noun, plural',\n",
        "    'NNP': 'Proper noun, singular',\n",
        "    'NNPS': 'Proper noun, plural',\n",
        "    'PDT': 'Predeterminer',\n",
        "    'POS': 'Possessive ending',\n",
        "    'PRP': 'Personal pronoun',\n",
        "    'PRP$': 'Possessive pronoun',\n",
        "    'RB': 'Adverb',\n",
        "    'RBR': 'Adverb, comparative',\n",
        "    'RBS': 'Adverb, superlative',\n",
        "    'RP': 'Particle',\n",
        "    'SYM': 'Symbol',\n",
        "    'TO': 'to',\n",
        "    'UH': 'Interjection',\n",
        "    'VB': 'Verb, base form',\n",
        "    'VBD': 'Verb, past tense',\n",
        "    'VBG': 'Verb, gerund or present participle',\n",
        "    'VBN': 'Verb, past participle',\n",
        "    'VBP': 'Verb, non-3rd person singular present',\n",
        "    'VBZ': 'Verb, 3rd person singular present',\n",
        "    'WDT': 'Wh-determiner',\n",
        "    'WP': 'Wh-pronoun',\n",
        "    'WP$': 'Possessive wh-pronoun',\n",
        "    'WRB': 'Wh-adverb'\n",
        "}\n"
      ],
      "metadata": {
        "id": "2WrbNXVRE72i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pos['POS Full Form'] = df_pos['POS'].map(pos_tag_full_form)\n",
        "\n",
        "# Display the DataFrame with the new column\n",
        "print(df_pos.head())"
      ],
      "metadata": {
        "id": "OmsOqYHXE-UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total occurrences of words for each POS tag\n",
        "pos_total_counts = df_pos.groupby('POS').size()\n",
        "\n",
        "# Display the total occurrences of words for each POS tag\n",
        "print(pos_total_counts)"
      ],
      "metadata": {
        "id": "1RBf-OtMF-tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Turn\n",
        "Conduct POS analysis for `sms_spam.csv` and `oct_delta.csv` data."
      ],
      "metadata": {
        "id": "pjx8eOqNL4Ss"
      }
    }
  ]
}