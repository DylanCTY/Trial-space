{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DylanCTY/Trial-space/blob/main/Lecture_6(0502).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Language Modeling\n",
        "Language modeling in natural language processing (NLP) plays a pivotal role in the development of intelligent systems that can understand and generate human language. Essentially, a language model aims to predict the likelihood of a sequence of words or the probability of the next word given a specific context. By capturing the underlying structure and patterns in textual data, language models facilitate various NLP tasks, such as machine translation, text summarization, sentiment analysis, and conversational AI.\n"
      ],
      "metadata": {
        "id": "iEgclihJUPC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Probabilistic Language Modeling with n-grams\n",
        "Probabilistic language modeling using n-grams is a fundamental approach in NLP that leverages the statistical properties of text to predict word sequences. An n-gram model represents text as contiguous sequences of n words, where the context for predicting the next word is limited to the previous n-1 words. For instance, a bigram (n=2) model restricts the context to a single preceding word, while a trigram (n=3) model considers the two preceding words.\n",
        "N-gram models estimate the probabilities of word sequences by calculating their frequency in a given corpus. They utilize the Markov assumption, which states that the probability of the next word depends only on the preceding n-1 words, thus simplifying computation.\n",
        "\n",
        "Despite their simplicity, n-gram models have been widely used in various NLP tasks, such as speech recognition, machine translation, and text generation. However, they have limitations, including data sparsity and the inability to capture long-range dependencies in text. The emergence of more sophisticated techniques like deep learning-based language models has shifted the focus, but n-gram models still hold relevance as a foundation for understanding language modeling and its development.\n",
        "\n",
        "In this session, we will explore n-grams-based language modeling. We will also explore language modeling using word embeddings.\n"
      ],
      "metadata": {
        "id": "xjh4t0WZUk9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: A Jane Austin Novel\n",
        "We will explore ‘next word prediction’ and ‘text generation’ based on the Jane Austin novel ‘Sense and Sensibility’. Let’s look first at all the books that are available.\n"
      ],
      "metadata": {
        "id": "Ee4joPHvVqyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View Books and Download"
      ],
      "metadata": {
        "id": "8fFPzF_QbRcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')  # Make sure the Gutenberg corpus is downloaded\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "mUhVAAMI4VXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7772a191-c18a-4ae3-8d39-5bc9a1bc35fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List available texts in the Gutenberg corpus\n",
        "print(gutenberg.fileids())"
      ],
      "metadata": {
        "id": "3f6uY6EUWHEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe0749b-9b6e-4734-c602-a58eb6edba22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load \"Sense and Sensibility\" text\n",
        "sas = gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Print the first 500 characters of \"Sense and Sensibility\"\n",
        "print(sas[:500])\n"
      ],
      "metadata": {
        "id": "ZWmkgwQIWnu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fddcd8-c617-4a16-f67e-54002ff34c00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Sense and Sensibility by Jane Austen 1811]\n",
            "\n",
            "CHAPTER 1\n",
            "\n",
            "\n",
            "The family of Dashwood had long been settled in Sussex.\n",
            "Their estate was large, and their residence was at Norland Park,\n",
            "in the centre of their property, where, for many generations,\n",
            "they had lived in so respectable a manner as to engage\n",
            "the general good opinion of their surrounding acquaintance.\n",
            "The late owner of this estate was a single man, who lived\n",
            "to a very advanced age, and who for many years of his life,\n",
            "had a constant companion an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##n-gram Model for Next Word Prediction and Text Generation.\n",
        "\n",
        "Following are the key steps.\n",
        "\n",
        "* **Step 1: Preprocess the Text**\n",
        "\n",
        "Start by tokenizing the text.\n",
        "\n",
        "* **Step 2: Build the N-gram Model**\n",
        "\n",
        "Create a trigram model, which will be used for predicting the next word based on the previous two words.\n",
        "\n",
        "* **Step 3: Next Word Prediction**\n",
        "\n",
        "Write a function that takes two words as input and predicts the most probable next word.\n",
        "\n",
        "* **Step 4: Text Generation**\n",
        "\n",
        "Using the trigram model, generate text by iteratively predicting the next word."
      ],
      "metadata": {
        "id": "W8cLDMiEZgot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, ngrams\n",
        "from collections import defaultdict, Counter\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sas.lower())  # Convert to lower case\n",
        "\n",
        "# Generate trigrams from the tokens\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "trigram_freq = defaultdict(Counter)\n",
        "\n",
        "# Populate the frequencies of trigrams\n",
        "for w1, w2, w3 in trigrams:\n",
        "    trigram_freq[(w1, w2)][w3] += 1\n",
        "\n",
        "# Function to predict the next word\n",
        "def predict_next_word(w1, w2):\n",
        "    if (w1, w2) in trigram_freq:\n",
        "        # Get the most common next word for the given bigram (w1, w2)\n",
        "        return trigram_freq[(w1, w2)].most_common(1)[0][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(start_words, num_words):\n",
        "    if len(start_words) < 2:\n",
        "        return \"Please provide at least two starting words.\"\n",
        "\n",
        "    generated_words = list(start_words)\n",
        "    for _ in range(num_words):\n",
        "        next_word = predict_next_word(generated_words[-2], generated_words[-1])\n",
        "        if next_word is None:\n",
        "            break  # Break if no next word is found\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Vxf3sp1ZhDc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "478f3d22-d489-49c2-fe50-460367bb0f82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Results"
      ],
      "metadata": {
        "id": "y9xSv4lVb0FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the prediction function\n",
        "print(\"Next word:\", predict_next_word('was', 'the'))"
      ],
      "metadata": {
        "id": "uuylj7PCafnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d369f4-a5ea-435a-c0f5-dd5844614269"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word: more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the text generation function\n",
        "start_words = (\"the\", \"more\")\n",
        "generate_text(start_words, 100)"
      ],
      "metadata": {
        "id": "94V121kEaNiF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "11151bbb-a543-4702-8eb6-a0d6e36d2754"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"the more easily reconciled , by the entrance of the house , and the two miss steeles , as she had been in the world . '' `` i am sure i would not be in town , and the two miss steeles , as she had been in the world . '' `` i am sure i would not be in town , and the two miss steeles , as she had been in the world . '' `` i am sure i would not be in town , and the two miss steeles , as she had been in the\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving Text Generation\n",
        "The approach of  picking the most frequent next word in text generation results in text repetition and uninteresting text.\n",
        "\n",
        "We will modify the `predict_next_word` function to choose the next word based on a probability distribution rather than just picking the most frequent next word. This way, the selection will still favor more likely words but won't always select the same word every time.\n"
      ],
      "metadata": {
        "id": "qUmqOYkycYnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to predict the next word with randomness\n",
        "def predict_next_word(w1, w2):\n",
        "    if (w1, w2) in trigram_freq:\n",
        "        next_words = list(trigram_freq[(w1, w2)].elements())\n",
        "        return random.choice(next_words) if next_words else None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to generate text with randomness\n",
        "def generate_text(start_words, num_words):\n",
        "    if len(start_words) < 2:\n",
        "        return \"Please provide at least two starting words.\"\n",
        "\n",
        "    generated_words = list(start_words)\n",
        "    for _ in range(num_words):\n",
        "        next_word = predict_next_word(generated_words[-2], generated_words[-1])\n",
        "        if next_word is None:\n",
        "            break  # Break if no next word is found\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "1njN8TtucZMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the text generation function\n",
        "start_words = (\"it\", \"was\")\n",
        "generate_text(start_words, 500)"
      ],
      "metadata": {
        "id": "vJRIisIOdMZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "We will use the `gensim` library, which provides straightforward implementations of `word2vec`, to create word embeddings using the CBOW model."
      ],
      "metadata": {
        "id": "WcvwuYiDf-SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize and convert to lower case\n",
        "tokens = word_tokenize(sas.lower())\n",
        "\n",
        "# Organize the tokens into sentences, Word2Vec needs data in format of list of lists of tokens\n",
        "sentences = [tokens[i:i+100] for i in range(0, len(tokens), 100)]\n"
      ],
      "metadata": {
        "id": "52rocFVboYXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the CBOW model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # sg=0 specifies CBOW\n"
      ],
      "metadata": {
        "id": "HO3VOV1Coa4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find 'Semantically Close' Words\n",
        "\n",
        "The `most_simila`r method is a feature of Gensim's Word2Vec implementation and is commonly used in natural language processing to find words that are most similar to a given word or set of words based on word embeddings. The method essentially calculates the cosine similarity between the specified word(s) and all other words in the model's vocabulary.\n",
        "\n",
        "Understanding Cosine Similarity\n",
        "\n",
        "Cosine similarity measures the cosine of the angle between two vectors. In the context of word embeddings, it is a measure of the similarity between two words. The cosine similarity between two vectors\n",
        "$u$ and $v$ is defined as:\n",
        "\n",
        "\\\n",
        "$$\n",
        "similarity = \\cos(\\theta) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n",
        "$$\n",
        "\n",
        "\n",
        "where $u \\cdot v$  is the dot product of the vectors and $\\|u\\|$  and $\\|v\\|$ are the norms (or magnitudes) of the vectors.\n",
        "\n",
        "\n",
        "Steps Performed by `most_similar`\n",
        "\n",
        "* Vector Retrieval: First, the method retrieves the vector(s) for the input word(s). If multiple words are provided, it typically averages their vectors to create a single query vector.\n",
        "\n",
        "* Similarity Calculation: The method then calculates the cosine similarity between this query vector and all other vectors in the trained model (i.e., the embeddings of all words in the vocabulary).\n",
        "\n",
        "* Sorting: The cosine similarities are sorted in descending order.\n",
        "\n",
        "* Top N Results: Finally, the method returns the top N words with the highest cosine similarities.\n",
        "\n",
        "Optional Parameters\n",
        "\n",
        "* Positive and Negative Words: The method allows for both positive and negative words. Positive words contribute positively towards the similarity, while negative words contribute negatively. This is useful for analogy tasks (e.g., \"king\" - \"man\" + \"woman\" = \"queen\").\n",
        "\n",
        "* Top N: You can specify how many of the most similar words to retrieve."
      ],
      "metadata": {
        "id": "PqNj34cM1oAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_words(input_word, topn=10):\n",
        "    if input_word in model.wv.key_to_index:\n",
        "        # Find the topn most similar words\n",
        "        similar_words = model.wv.most_similar(positive=[input_word], topn=topn)\n",
        "        return similar_words  # This returns a list of tuples (word, similarity)\n",
        "    else:\n",
        "        return f\"The word '{input_word}' is not in the vocabulary.\""
      ],
      "metadata": {
        "id": "x1U7ihQ6pR_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the function\n",
        "find_closest_words(\"sense\", 10)"
      ],
      "metadata": {
        "id": "CSxmA2BL1vZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-D Plot\n",
        "\n",
        "Steps:\n",
        "* Retrieve Closest Words: Get the 20 closest words to the input word using the most_similar method.\n",
        "\n",
        "* Extract Embeddings: Extract the embedding vectors for these words.\n",
        "\n",
        "* Apply t-SNE: Use t-SNE to reduce the dimensionality of these vectors to two dimensions. Dimensionality reduction techniques such as t-SNE (t-Distributed Stochastic Neighbor Embedding) or PCA (Principal Component Analysis) can be used. We will use t-SNE here because it is particularly well-suited for visualizing high-dimensional data in two or three dimensions.\n",
        "\n",
        "* Plotting: Plot the resulting two-dimensional points using matplotlib."
      ],
      "metadata": {
        "id": "4E1icxV912XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "def plot_closest_words(model, input_word, topn=20):\n",
        "    if input_word not in model.wv.key_to_index:\n",
        "        print(f\"The word '{input_word}' is not in the vocabulary.\")\n",
        "        return\n",
        "\n",
        "    # Get the closest words specified by 'topn'\n",
        "    closest_words = model.wv.most_similar(positive=[input_word], topn=topn)\n",
        "    words = [word for word, _ in closest_words]\n",
        "    words.append(input_word)  # Also include the input word\n",
        "\n",
        "    # Extract the corresponding vectors\n",
        "    word_vectors = np.array([model.wv[word] for word in words])\n",
        "\n",
        "    # Use t-SNE to reduce dimensionality\n",
        "    perplexity_value = min(30, len(words) - 1)  # Ensure perplexity is less than the number of samples\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
        "    Y = tsne.fit_transform(word_vectors)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.scatter(Y[:, 0], Y[:, 1], color='blue')\n",
        "\n",
        "    for label, x, y in zip(words, Y[:, 0], Y[:, 1]):\n",
        "        plt.annotate(label,\n",
        "                     xy=(x, y),\n",
        "                     xytext=(5, -5),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "OPqq2c96rfM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the function\n",
        "plot_closest_words(model, \"life\", topn=20)"
      ],
      "metadata": {
        "id": "LBW9gETUriyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3-D Plot"
      ],
      "metadata": {
        "id": "VDjJYbeA17_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # This is needed for 3D plotting\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "def plot_closest_words_3d(model, input_word, topn=100):\n",
        "    if input_word not in model.wv.key_to_index:\n",
        "        print(f\"The word '{input_word}' is not in the vocabulary.\")\n",
        "        return\n",
        "\n",
        "    # Get the closest words specified by 'topn'\n",
        "    closest_words = model.wv.most_similar(positive=[input_word], topn=topn)\n",
        "    words = [word for word, _ in closest_words]\n",
        "    words.append(input_word)  # Also include the input word\n",
        "\n",
        "    # Extract the corresponding vectors\n",
        "    word_vectors = np.array([model.wv[word] for word in words])\n",
        "\n",
        "    # Use t-SNE to reduce dimensionality to 3D\n",
        "    perplexity_value = min(30, len(words) - 1)  # Ensure perplexity is less than the number of samples\n",
        "    tsne = TSNE(n_components=3, random_state=42, perplexity=perplexity_value)\n",
        "    Y = tsne.fit_transform(word_vectors)\n",
        "\n",
        "    # Plotting in 3D\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], color='blue')\n",
        "\n",
        "    for label, x, y, z in zip(words, Y[:, 0], Y[:, 1], Y[:, 2]):\n",
        "        ax.text(x, y, z, label, color='red', fontsize=9)\n",
        "\n",
        "    ax.set_xlabel('t-SNE Axis 1')\n",
        "    ax.set_ylabel('t-SNE Axis 2')\n",
        "    ax.set_zlabel('t-SNE Axis 3')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ADWxtuZVrqcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the function\n",
        "plot_closest_words_3d(model, \"present\", topn=20)"
      ],
      "metadata": {
        "id": "P5V0pFd6sDDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next Word Prediction with Embeddings"
      ],
      "metadata": {
        "id": "wPkTTNte1McQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we will attempt to use the last two words to predict the next word. But this time we will use the embeddings of the last two words."
      ],
      "metadata": {
        "id": "bbFvk_8Oq0cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Data\n",
        "Because embeddings are numeric vectors, it allows us to build a prediction model for the next word prediction. We need to prepare the training data for the prediction model. This involves:\n",
        "\n",
        "1. Creating sequences of embeddings for pairs of words to use as inputs.\n",
        "2. Using the embedding of the word that immediately follows each pair as the target output."
      ],
      "metadata": {
        "id": "GzMKS5PB99w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Create a dictionary to map words to their embeddings\n",
        "word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "\n",
        "# Preparing the data for next word prediction\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    for i in range(len(sentence) - 2):\n",
        "        if sentence[i] in word_vectors and sentence[i+1] in word_vectors and sentence[i+2] in word_vectors:\n",
        "            first_word_vec = word_vectors[sentence[i]]\n",
        "            second_word_vec = word_vectors[sentence[i+1]]\n",
        "            next_word_vec = word_vectors[sentence[i+2]]\n",
        "            X.append(np.concatenate([first_word_vec, second_word_vec]))\n",
        "            y.append(next_word_vec)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n"
      ],
      "metadata": {
        "id": "tqUnkhWQoT5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network Design"
      ],
      "metadata": {
        "id": "Ej8dLfrV9o9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the following neural network structure:\n",
        "1. Input Layer: The input dimension will be twice the `vector_size` of the embeddings (since we concatenate two embeddings).\n",
        "2. Hidden Layers: One or more dense layers with activation functions like ReLU.\n",
        "3. Output Layer: The output layer should have as many units as the `vector_size` of the embeddings and could use a linear activation function.\n",
        "4. Train the Model: The model learns to predict the vector of the next word directly."
      ],
      "metadata": {
        "id": "yceSOQe3rETC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_dim=200),  # Assuming vector_size=100\n",
        "    Dense(100, activation='linear')  # Output dimension same as vector_size\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X, y, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "id": "k6poWgkCreRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to Find Closest Word\n",
        "\n",
        "We will create a function to find the closest word in the embedding space. This function will compare the predicted embedding to all embeddings in  trained Word2Vec model,  using cosine similarity to determine the closest word."
      ],
      "metadata": {
        "id": "b5ld9tAerl1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "def closest_word(predicted_embedding, word_vectors):\n",
        "    min_dist = float('inf')\n",
        "    closest_word = None\n",
        "    for word, embedding in word_vectors.items():\n",
        "        sim = distance.cosine(predicted_embedding, embedding)\n",
        "        if sim < min_dist:\n",
        "            min_dist = sim\n",
        "            closest_word = word\n",
        "    return closest_word\n"
      ],
      "metadata": {
        "id": "gRla9f-NqDhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction Function\n",
        "We now create a function that takes the last two words, predicts the next word's embedding using the trained model, and then uses the function above to find the closest actual word."
      ],
      "metadata": {
        "id": "PKvPOfQ4r9-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(word1, word2, model, word_vectors):\n",
        "    if word1 not in word_vectors or word2 not in word_vectors:\n",
        "        return \"One of the words is not in the vocabulary.\"\n",
        "\n",
        "    # Retrieve the embeddings from word_vectors\n",
        "    first_word_vec = word_vectors[word1]\n",
        "    second_word_vec = word_vectors[word2]\n",
        "\n",
        "    # Combine embeddings and reshape for the model\n",
        "    combined_embeddings = np.concatenate([first_word_vec, second_word_vec]).reshape(1, -1)\n",
        "\n",
        "    # Predict the next word's embedding\n",
        "    predicted_embedding = model.predict(combined_embeddings)\n",
        "\n",
        "    # Find the closest word in the embedding space\n",
        "    next_word = closest_word(predicted_embedding.flatten(), word_vectors)\n",
        "    return next_word\n"
      ],
      "metadata": {
        "id": "RQAbW0H_pgyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predict the Next Word"
      ],
      "metadata": {
        "id": "UMhPKSdF6MWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example words\n",
        "word1 = \"he\"\n",
        "word2 = \"was\"\n",
        "\n",
        "# Assuming 'model' is your trained Keras model and 'word_vectors' is a dictionary of word embeddings\n",
        "predicted_word = predict_next_word(word1, word2, model, word_vectors)\n",
        "print(\"Predicted next word:\", predicted_word)\n"
      ],
      "metadata": {
        "id": "1ziw7_7tpmBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation (Deterministic)"
      ],
      "metadata": {
        "id": "0k1qwrNc6fwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(start_words, num_words, model, word_vectors):\n",
        "    generated_words = start_words.split()\n",
        "    for _ in range(num_words):\n",
        "        if len(generated_words) < 2:\n",
        "            break  # If there aren't enough words to predict the next one, stop.\n",
        "        # Extract the last two words\n",
        "        last_two_words = generated_words[-2:]\n",
        "        # Ensure that there are exactly two words passed\n",
        "        next_word = predict_next_word(last_two_words[0], last_two_words[1], model, word_vectors)\n",
        "        generated_words.append(next_word)\n",
        "    return ' '.join(generated_words)\n"
      ],
      "metadata": {
        "id": "d-eWbNgdojja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = \"sense and\"\n",
        "num_words = 50\n",
        "# Make sure your 'model' and 'word_vectors' are defined and loaded appropriately.\n",
        "generated_text = generate_text(start_words, num_words, model, word_vectors)"
      ],
      "metadata": {
        "id": "8juKgp471Ef4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "id": "kKeaRXDh60IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation (Stochastic)"
      ],
      "metadata": {
        "id": "_PPVsiZ_636t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To introduce randomness into the text generation process and avoid generating repetitive text, we will modify how we select the next word after generating its predicted embedding. Rather than always choosing the closest word in terms of embedding distance, will select from the top\n",
        "𝑁 closest words, weighted by their similarity to the predicted embedding. This approach can lead to more varied and interesting text."
      ],
      "metadata": {
        "id": "vQ4Itrp33lmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def predict_next_word(word1, word2, model, word_vectors, top_n=20):\n",
        "    if word1 not in word_vectors or word2 not in word_vectors:\n",
        "        return \"One of the words is not in the vocabulary.\"\n",
        "\n",
        "    first_word_vec = word_vectors[word1]\n",
        "    second_word_vec = word_vectors[word2]\n",
        "    combined_embeddings = np.concatenate([first_word_vec, second_word_vec]).reshape(1, -1)\n",
        "    predicted_embedding = model.predict(combined_embeddings).flatten()\n",
        "\n",
        "    # Get the top N closest words\n",
        "    top_words, similarities = closest_words(predicted_embedding, word_vectors, top_n)\n",
        "\n",
        "    # Convert similarities to positive as distances are negative, then to probabilities\n",
        "    probabilities = np.exp(-similarities)\n",
        "    probabilities /= probabilities.sum()\n",
        "\n",
        "    # Randomly select a word based on computed probabilities\n",
        "    next_word = np.random.choice(top_words, p=probabilities)\n",
        "    return next_word\n"
      ],
      "metadata": {
        "id": "SC-FDyhu1obA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "\n",
        "def closest_words(predicted_embedding, word_vectors, top_n=20):\n",
        "    # Calculate distances and return the top N closest words\n",
        "    all_words = list(word_vectors.keys())\n",
        "    all_embeddings = np.array([word_vectors[word] for word in all_words])\n",
        "    distances = np.array([distance.cosine(predicted_embedding, embedding) for embedding in all_embeddings])\n",
        "    closest_indices = np.argsort(distances)[:top_n]\n",
        "    closest_words = [all_words[idx] for idx in closest_indices]\n",
        "    return closest_words, distances[closest_indices]\n"
      ],
      "metadata": {
        "id": "owTV-ti11lu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(start_words, num_words, model, word_vectors, top_n=20):\n",
        "    generated_words = start_words.split()\n",
        "    for _ in range(num_words):\n",
        "        if len(generated_words) < 2:\n",
        "            break\n",
        "        last_two_words = generated_words[-2:]\n",
        "        next_word = predict_next_word(last_two_words[0], last_two_words[1], model, word_vectors, top_n)\n",
        "        generated_words.append(next_word)\n",
        "    return ' '.join(generated_words)\n"
      ],
      "metadata": {
        "id": "dJ4oCz5M1oYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_words = \"sense and\"\n",
        "num_words = 50\n",
        "# Make sure your 'model' and 'word_vectors' are defined and loaded appropriately.\n",
        "generated_text = generate_text(start_words, num_words, model, word_vectors)"
      ],
      "metadata": {
        "id": "hle2Icwa7o_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "id": "dO90x-rx7qNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that the quality of the generated text depends on the quality of the word embeddings used. The word embeddings in this example were trained on a small dataset using the CBOW word2vec model with a low-dimensional embedding space. As a result, the generated text may not be as coherent as expected.\n",
        "\n",
        "As the code above illustrates, both next word prediction and text generation tasks are complicated and these algorithms perform rather poorly. This is due to their inability to capture long-range dependencies in text. Large language models (LLMs) can address these limitations, but come at a huge cost in terms of training and computation.\n"
      ],
      "metadata": {
        "id": "_oLDD-Jc2PAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your turn\n",
        "\n",
        "1. Create next word prediction and text generation using n-grams based on the novel Moby Dick, by Herman Melville. You can access it with `melville-moby_dick.txt`.\n",
        "2. Create embeddings based on the file `tinyshakespeare.txt`.Find the closest words to 'king' and plot the results."
      ],
      "metadata": {
        "id": "x_fhW1xr2ijg"
      }
    }
  ]
}